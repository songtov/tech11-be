#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AXPress Scholar Agent - arXiv API ê¸°ë°˜ ë…¼ë¬¸ ê²€ìƒ‰ ë° ì¶”ì²œ ì‹œìŠ¤í…œ
íŠ¹ì • ë„ë©”ì¸ì˜ ìµœì‹  ë…¼ë¬¸ê³¼ ì¸ê¸° ë…¼ë¬¸ì„ ì¶”ì²œí•˜ê³  PDF ë‹¤ìš´ë¡œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import requests
import json
import os
import sys
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Paper:
    """ë…¼ë¬¸ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    title: str
    authors: List[str]
    published_date: str
    updated_date: str
    abstract: str
    categories: List[str]
    pdf_url: str
    arxiv_url: str
    citation_count: int = 0
    relevance_score: float = 0.0

class AXPressScholarAgent:
    """arXiv APIë¥¼ í™œìš©í•œ ë…¼ë¬¸ ê²€ìƒ‰ ë° ì¶”ì²œ ì—ì´ì „íŠ¸"""
    
    def __init__(self, download_dir: str = "downloaded_papers"):
        self.base_url = "http://export.arxiv.org/api/query"
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(exist_ok=True)
        
        # ë„ë©”ì¸ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ë§¤í•‘
        self.domain_keywords = {
            "ì œì¡°": ["manufacturing", "production", "industrial", "factory", "automation", "robotics"],
            "ê¸ˆìœµ": ["finance", "financial", "banking", "fintech", "investment", "trading", "economics"],
            "CLOUD": ["cloud computing", "distributed systems", "microservices", "kubernetes", "container"],
            "í†µì‹ ": ["telecommunications", "communication", "network", "5G", "6G", "wireless"],
            "ìœ í†µ/ë¬¼ë¥˜": ["logistics", "supply chain", "distribution", "retail", "e-commerce", "optimization"],
            "Gen AI": ["artificial intelligence", "machine learning", "deep learning", "LLM", "generative AI", "neural networks"]
        }
        
        # ë„ë©”ì¸ë³„ ìœ ëª… ì €ë„ ì •ë³´ (ì¸ìš©ìˆ˜ ê¸°ë°˜ ê²€ìƒ‰ìš©)
        self.domain_journals = {
            "ê¸ˆìœµ": ["Journal of Finance", "Journal of Financial Economics", "Review of Financial Studies", "Science"],
            "í†µì‹ ": ["IEEE Communications Magazine", "IEEE Transactions on Communications", "Science"],
            "ì œì¡°": ["Journal of Manufacturing Systems", "CIRP Annals", "Science"],
            "ìœ í†µ/ë¬¼ë¥˜": ["Transportation Research Part E", "International Journal of Physical Distribution & Logistics Management"],
            "Gen AI": ["AI Journal", "Journal of Artificial Intelligence Research", "NeurIPS", "ICML", "ICLR", "Science"],
            "CLOUD": ["IEEE Transactions on Cloud Computing", "Springer Journal of Cloud Computing", "Science"]
        }
        
        # arXiv ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë” ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´)
        self.arxiv_categories = {
            "ì œì¡°": ["cs.RO", "cs.SY", "eess.SY"],  # Robotics, Systems, Control Systems
            "ê¸ˆìœµ": ["q-fin.GN", "q-fin.CP", "econ.GN"],  # General Finance, Computational Finance
            "CLOUD": ["cs.DC", "cs.DS", "cs.SE"],  # Distributed Computing, Data Structures, Software Engineering
            "í†µì‹ ": ["cs.NI", "eess.SP"],  # Networking, Signal Processing
            "ìœ í†µ/ë¬¼ë¥˜": ["cs.AI", "math.OC"],  # Artificial Intelligence, Optimization
            "Gen AI": ["cs.AI", "cs.LG", "cs.CL"]  # AI, Machine Learning, Computation and Language
        }
    
    def fetch_papers(self, domain: str) -> List[Paper]:
        """ì§€ì •ëœ ë„ë©”ì¸ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. (2ë…„ ë‚´ ì¸ìš©ìˆ˜ ë†’ì€ 5í¸)"""
        logger.info(f"ë„ë©”ì¸ '{domain}'ì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘ (ê³ ì¸ìš© ë…¼ë¬¸ ëª¨ë“œ)")
        
        # ìƒˆë¡œìš´ ê³ ì¸ìš© ë…¼ë¬¸ ê²€ìƒ‰ ê¸°ëŠ¥ ì‚¬ìš©
        return self.fetch_highly_cited_papers(domain)
    
    
    def fetch_highly_cited_papers(self, domain: str) -> List[Paper]:
        """ë„ë©”ì¸ë³„ ìœ ëª… ì €ë„ì—ì„œ 2ë…„ ë‚´ ì¸ìš©ìˆ˜ê°€ ê°€ì¥ ë†’ì€ 5ê°œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  arXivì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        logger.info(f"ë„ë©”ì¸ '{domain}'ì—ì„œ ê³ ì¸ìš© ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘ (Semantic Scholar + arXiv)")
        
        if domain not in self.domain_keywords:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë„ë©”ì¸ì…ë‹ˆë‹¤: {domain}")
        
        try:
            # 2ë…„ ì „ ë‚ ì§œ ê³„ì‚°
            two_years_ago = datetime.now() - timedelta(days=730)
            
            # ë„ë©”ì¸ë³„ ì €ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            journals = self.domain_journals.get(domain, [])
            if not journals:
                logger.warning(f"ë„ë©”ì¸ '{domain}'ì— ëŒ€í•œ ì €ë„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                return self.fetch_papers(domain)
            
            all_papers = []
            
            # ê° ì €ë„ë³„ë¡œ Semantic Scholar APIë¡œ ê²€ìƒ‰
            for journal in journals:
                logger.info(f"ì €ë„ '{journal}'ì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...")
                
                # Semantic Scholar APIë¡œ ê²€ìƒ‰ (ëª¨ë“  ì—°ë„)
                journal_papers = self._search_semantic_scholar(journal, two_years_ago)
                all_papers.extend(journal_papers)
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                time.sleep(1.0)
            
            if not all_papers:
                logger.warning("Semantic Scholarì—ì„œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                return self.fetch_papers(domain)
            
            # 2ë…„ ë‚´ ë…¼ë¬¸ë§Œ í•„í„°ë§
            recent_papers = []
            for paper in all_papers:
                try:
                    # published_dateì—ì„œ ì—°ë„ ì¶”ì¶œ
                    if paper.published_date:
                        paper_year = int(paper.published_date.split('-')[0])
                        if paper_year >= two_years_ago.year:
                            recent_papers.append(paper)
                except:
                    continue
            
            if not recent_papers:
                logger.warning("2ë…„ ë‚´ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                return self.fetch_papers(domain)
            
            # ì¸ìš©ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (2ë…„ ë‚´ ë…¼ë¬¸ ì¤‘ì—ì„œ)
            recent_papers.sort(key=lambda x: x.citation_count, reverse=True)
            
            # ìƒìœ„ 5ê°œ ì„ íƒ (2ë…„ ë‚´ ì¸ìš©ìˆ˜ ê°€ì¥ ë†’ì€ ë…¼ë¬¸ë“¤)
            selected_papers = recent_papers[:5]
            
            logger.info(f"2ë…„ ë‚´ ì¸ìš©ìˆ˜ ê¸°ì¤€ ìƒìœ„ {len(selected_papers)}í¸ì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            # ì´ì œ arXivì—ì„œ í•´ë‹¹ ë…¼ë¬¸ë“¤ì„ ê²€ìƒ‰í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ë²„ì „ì„ ì°¾ìŠµë‹ˆë‹¤
            arxiv_papers = []
            for paper in selected_papers:
                logger.info(f"arXivì—ì„œ '{paper.title}' ê²€ìƒ‰ ì¤‘...")
                arxiv_paper = self._search_arxiv_by_title(paper.title)
                if arxiv_paper:
                    # Semantic Scholarì—ì„œ ê°€ì ¸ì˜¨ ì¸ìš©ìˆ˜ ì •ë³´ë¥¼ ìœ ì§€
                    arxiv_paper.citation_count = paper.citation_count
                    arxiv_papers.append(arxiv_paper)
                    logger.info(f"arXivì—ì„œ ë°œê²¬: {arxiv_paper.title}")
                else:
                    logger.warning(f"arXivì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper.title}")
                    # arXivì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë…¼ë¬¸ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    arxiv_papers.append(paper)
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                time.sleep(0.5)
            
            logger.info(f"ìµœì¢… {len(arxiv_papers)}í¸ì˜ ë…¼ë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return arxiv_papers
            
        except Exception as e:
            logger.error(f"ê³ ì¸ìš© ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ fallback
            return self.fetch_papers(domain)
    
    def _search_semantic_scholar(self, journal: str, min_date: datetime) -> List[Paper]:
        """Semantic Scholar APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì €ë„ì˜ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # Semantic Scholar API ì—”ë“œí¬ì¸íŠ¸
            base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ë” ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ë³€ê²½)
            query = journal  # venue: ì œê±°í•˜ê³  ì €ë„ëª…ë§Œ ì‚¬ìš©
            
            params = {
                'query': query,
                'limit': 50,  # ì œí•œì„ ì¤„ì—¬ì„œ ì•ˆì •ì„± í–¥ìƒ
                'fields': 'paperId,title,authors,year,venue,citationCount,abstract,isOpenAccess,openAccessPdf,externalIds'
            }
            
            logger.info(f"Semantic Scholar API ìš”ì²­: {params}")
            
            # User-Agent í—¤ë” ì¶”ê°€
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(base_url, params=params, headers=headers, timeout=30)
            logger.info(f"API ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            
            if response.status_code != 200:
                logger.error(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return []
            
            response.raise_for_status()
            
            data = response.json()
            logger.info(f"API ì‘ë‹µ ë°ì´í„°: {len(data.get('data', []))}ê°œ ë…¼ë¬¸ ë°œê²¬")
            
            papers = []
            
            for paper_data in data.get('data', []):
                try:
                    # ë…¼ë¬¸ì´ í•´ë‹¹ ì €ë„ì—ì„œ ë‚˜ì˜¨ ê²ƒì¸ì§€ í™•ì¸
                    paper_venue = paper_data.get('venue', '').lower()
                    journal_lower = journal.lower()
                    
                    # ì €ë„ëª…ì´ venueì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if journal_lower not in paper_venue and not any(word in paper_venue for word in journal_lower.split()):
                        continue
                    
                    # ëª¨ë“  ì—°ë„ì˜ ë…¼ë¬¸ì„ ê°€ì ¸ì˜¨ í›„ ë‚˜ì¤‘ì— í•„í„°ë§
                    paper_year = paper_data.get('year', 0)
                    
                    # arXiv IDê°€ ìˆëŠ”ì§€ í™•ì¸
                    arxiv_id = None
                    external_ids = paper_data.get('externalIds', {})
                    if external_ids:
                        arxiv_id = external_ids.get('ArXiv')
                    
                    # ì €ì ì •ë³´ ì¶”ì¶œ
                    authors = []
                    for author in paper_data.get('authors', []):
                        if author.get('name'):
                            authors.append(author['name'])
                    
                    # PDF URL ìƒì„± (arXiv ìš°ì„ , ê·¸ ë‹¤ìŒ openAccessPdf)
                    pdf_url = ""
                    if arxiv_id:
                        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                    elif paper_data.get('openAccessPdf', {}).get('url'):
                        pdf_url = paper_data['openAccessPdf']['url']
                    
                    # arXiv URL ìƒì„±
                    arxiv_url = f"https://arxiv.org/abs/{arxiv_id}" if arxiv_id else ""
                    
                    # ë°œí‘œì¼ ì²˜ë¦¬ (yearë§Œ ì‚¬ìš©)
                    published_date = f"{paper_year}-01-01T00:00:00Z"
                    updated_date = f"{paper_year}-01-01T00:00:00Z"
                    
                    # Paper ê°ì²´ ìƒì„±
                    paper = Paper(
                        id=paper_data.get('paperId', ''),
                        title=paper_data.get('title', 'No Title'),
                        authors=authors,
                        published_date=published_date,
                        updated_date=updated_date,
                        abstract=paper_data.get('abstract', ''),
                        categories=[journal],  # ì €ë„ëª…ì„ ì¹´í…Œê³ ë¦¬ë¡œ ì‚¬ìš©
                        pdf_url=pdf_url,
                        arxiv_url=arxiv_url,
                        citation_count=paper_data.get('citationCount', 0),
                        relevance_score=0.0
                    )
                    
                    papers.append(paper)
                    
                except Exception as e:
                    logger.warning(f"ë…¼ë¬¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"ì €ë„ '{journal}'ì—ì„œ {len(papers)}í¸ì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return papers
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Semantic Scholar API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []
        except Exception as e:
            logger.error(f"Semantic Scholar ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _search_arxiv_by_title(self, title: str) -> Optional[Paper]:
        """ì œëª©ìœ¼ë¡œ arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # arXiv API ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = f'ti:"{title}"'
            
            params = {
                'search_query': search_query,
                'start': 0,
                'max_results': 5,  # ìµœëŒ€ 5ê°œ ê²°ê³¼
                'sortBy': 'relevance',
                'sortOrder': 'descending'
            }
            
            logger.info(f"arXiv ì œëª© ê²€ìƒ‰: {search_query}")
            
            # arXiv API ìš”ì²­
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # XML íŒŒì‹±
            from xml.etree import ElementTree as ET
            root = ET.fromstring(response.content)
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            entries = root.findall('atom:entry', ns)
            
            # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë…¼ë¬¸ ë°˜í™˜
            if entries:
                paper = self._parse_arxiv_entry(entries[0], ns)
                if paper:
                    logger.info(f"arXivì—ì„œ ë…¼ë¬¸ ë°œê²¬: {paper.title}")
                    return paper
            
            return None
            
        except Exception as e:
            logger.error(f"arXiv ì œëª© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _search_latest_papers(self, domain: str, max_results: int = 10) -> List[Paper]:
        """ìµœì‹  ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        keywords = self.domain_keywords[domain]
        categories = self.arxiv_categories.get(domain, [])
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = " OR ".join([f'all:{keyword}' for keyword in keywords])
        if categories:
            category_query = " OR ".join([f'cat:{cat}' for cat in categories])
            search_query = f"({search_query}) OR ({category_query})"
        
        # 1ë…„ ì „ ë‚ ì§œ ê³„ì‚°
        one_year_ago = datetime.now() - timedelta(days=365)
        date_filter = one_year_ago.strftime("%Y%m%d")
        
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': max_results * 2,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        return self._make_arxiv_request(params, max_results)
    
    def _search_popular_paper(self, domain: str) -> Optional[Paper]:
        """ì¸ê¸° ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (relevance ê¸°ì¤€)."""
        keywords = self.domain_keywords[domain]
        categories = self.arxiv_categories.get(domain, [])
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = " OR ".join([f'all:{keyword}' for keyword in keywords])
        if categories:
            category_query = " OR ".join([f'cat:{cat}' for cat in categories])
            search_query = f"({search_query}) OR ({category_query})"
        
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': 20,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }
        
        papers = self._make_arxiv_request(params, 1)
        return papers[0] if papers else None
    
    def _make_arxiv_request(self, params: Dict, max_results: int) -> List[Paper]:
        """arXiv APIì— ìš”ì²­ì„ ë³´ë‚´ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"arXiv API ìš”ì²­: {params}")
            
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # XML íŒŒì‹±
            from xml.etree import ElementTree as ET
            root = ET.fromstring(response.content)
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            papers = []
            entries = root.findall('atom:entry', ns)
            
            for entry in entries[:max_results]:
                paper = self._parse_arxiv_entry(entry, ns)
                if paper:
                    papers.append(paper)
            
            return papers
            
        except requests.exceptions.RequestException as e:
            logger.error(f"arXiv API ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def _parse_arxiv_entry(self, entry, ns: Dict) -> Optional[Paper]:
        """arXiv ì—”íŠ¸ë¦¬ë¥¼ íŒŒì‹±í•˜ì—¬ Paper ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # ID ì¶”ì¶œ
            id_elem = entry.find('atom:id', ns)
            paper_id = id_elem.text.strip() if id_elem is not None else "unknown"
            
            # ì œëª© ì¶”ì¶œ
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text.strip() if title_elem is not None else "No Title"
            
            # ì €ì ì¶”ì¶œ
            authors = []
            for author in entry.findall('atom:author', ns):
                name_elem = author.find('atom:name', ns)
                if name_elem is not None:
                    authors.append(name_elem.text.strip())
            
            # ë‚ ì§œ ì¶”ì¶œ
            published_elem = entry.find('atom:published', ns)
            published_date = published_elem.text.strip() if published_elem is not None else ""
            
            updated_elem = entry.find('atom:updated', ns)
            updated_date = updated_elem.text.strip() if updated_elem is not None else ""
            
            # ìš”ì•½ ì¶”ì¶œ
            summary_elem = entry.find('atom:summary', ns)
            abstract = summary_elem.text.strip() if summary_elem is not None else ""
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            categories = []
            for category in entry.findall('atom:category', ns):
                term = category.get('term')
                if term:
                    categories.append(term)
            
            # PDF URL ìƒì„±
            pdf_url = f"https://arxiv.org/pdf/{paper_id.split('/')[-1]}.pdf"
            arxiv_url = paper_id
            
            return Paper(
                id=paper_id,
                title=title,
                authors=authors,
                published_date=published_date,
                updated_date=updated_date,
                abstract=abstract,
                categories=categories,
                pdf_url=pdf_url,
                arxiv_url=arxiv_url
            )
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def display_papers(self, papers: List[Paper]) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        if not papers:
            print("ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*80)
        print("ğŸ“š AXPress Scholar Agent - ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼")
        print("="*80)
        
        for i, paper in enumerate(papers, 1):
            print(f"\n{i}. {paper.title}")
            print(f"   ì €ì: {', '.join(paper.authors[:3])}{' ì™¸' if len(paper.authors) > 3 else ''}")
            
            # ë‚ ì§œ í¬ë§·íŒ…
            try:
                pub_date = datetime.fromisoformat(paper.published_date.replace('Z', '+00:00'))
                formatted_date = pub_date.strftime("%Y-%m-%d")
                print(f"   ë°œí‘œì¼: {formatted_date}")
            except:
                print(f"   ë°œí‘œì¼: {paper.published_date}")
            
            print(f"   ì¹´í…Œê³ ë¦¬: {', '.join(paper.categories[:3])}")
            
            # ì¸ìš©ìˆ˜ í‘œì‹œ (Semantic Scholarì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°)
            if paper.citation_count > 0:
                print(f"   ğŸ“Š ì¸ìš©ìˆ˜: {paper.citation_count}")
            
            print(f"   PDF: [PDF Available] - {paper.pdf_url}")
            
            # ìš”ì•½ ë¯¸ë¦¬ë³´ê¸° (ì²« 100ì)
            if paper.abstract:
                preview = paper.abstract[:100] + "..." if len(paper.abstract) > 100 else paper.abstract
                print(f"   ìš”ì•½: {preview}")
    
    def download_pdf(self, paper: Paper) -> Optional[str]:
        """ë…¼ë¬¸ì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ëª… ìƒì„±
            safe_title = "".join(c for c in paper.title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title[:100]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
            filename = f"{safe_title}_{paper.id.split('/')[-1]}.pdf"
            filepath = self.download_dir / filename
            
            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if filepath.exists():
                logger.info(f"íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {filepath}")
                return str(filepath)
            
            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì‹œì‘: {paper.pdf_url}")
            print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {paper.title[:50]}...")
            
            # PDF ë‹¤ìš´ë¡œë“œ
            response = requests.get(paper.pdf_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            print(f"âœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            return str(filepath)
            
        except requests.exceptions.Timeout:
            logger.error("PDF ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            print("âŒ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            return None
        except requests.exceptions.ConnectionError:
            logger.error("PDF ë‹¤ìš´ë¡œë“œ ì—°ê²° ì˜¤ë¥˜")
            print("âŒ ì—°ê²° ì˜¤ë¥˜")
            return None
        except requests.exceptions.HTTPError as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ HTTP ì˜¤ë¥˜: {e}")
            print(f"âŒ HTTP ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”¬ AXPress Scholar Agent - arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("="*60)
    
    # ì§€ì› ë„ë©”ì¸ í‘œì‹œ
    domains = ["ì œì¡°", "ê¸ˆìœµ", "CLOUD", "í†µì‹ ", "ìœ í†µ/ë¬¼ë¥˜", "Gen AI"]
    print("\nğŸ“‹ ì§€ì› ë„ë©”ì¸:")
    for i, domain in enumerate(domains, 1):
        print(f"   {i}. {domain}")
    
    try:
        # ë„ë©”ì¸ ì„ íƒ
        while True:
            try:
                choice = input(f"\në„ë©”ì¸ì„ ì„ íƒí•˜ì„¸ìš” (1-{len(domains)}): ").strip()
                domain_index = int(choice) - 1
                
                if 0 <= domain_index < len(domains):
                    selected_domain = domains[domain_index]
                    break
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì—ì´ì „íŠ¸ ìƒì„± ë° ë…¼ë¬¸ ê²€ìƒ‰
        agent = AXPressScholarAgent()
        print(f"\nğŸ” '{selected_domain}' ë„ë©”ì¸ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        
        # ìƒˆë¡œìš´ ê¸°ëŠ¥: ê³ ì¸ìš© ë…¼ë¬¸ ê²€ìƒ‰
        print("ğŸ“Š ê³ ì¸ìš© ë…¼ë¬¸ ê²€ìƒ‰ ëª¨ë“œ (2ë…„ ë‚´ ì¸ìš©ìˆ˜ ê¸°ì¤€)")
        papers = agent.fetch_highly_cited_papers(selected_domain)
        
        if not papers:
            print("ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê²°ê³¼ í‘œì‹œ
        agent.display_papers(papers)
        
        # PDF ë‹¤ìš´ë¡œë“œ ì„ íƒ
        print(f"\nğŸ“„ ì´ {len(papers)}í¸ì˜ ë…¼ë¬¸ ì¤‘ì—ì„œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•  ë…¼ë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.")
        print("ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (0: ì¢…ë£Œ): ", end="")
        
        try:
            choice = int(input().strip())
            if choice == 0:
                print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
            elif 1 <= choice <= len(papers):
                selected_paper = papers[choice - 1]
                print(f"\nì„ íƒëœ ë…¼ë¬¸: {selected_paper.title}")
                
                filepath = agent.download_pdf(selected_paper)
                
                if filepath:
                    print(f"\nâœ… PDF ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    print(f"ì €ì¥ ìœ„ì¹˜: {filepath}")
                else:
                    print("\nâŒ PDF ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# final.pyì™€ ì™„ì „íˆ ë™ì¼í•œ AXPressScholarAgent í´ë˜ìŠ¤
# ì´ì œ final.pyë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šê³ ë„ ë™ì¼í•œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

if __name__ == "__main__":
    main()
