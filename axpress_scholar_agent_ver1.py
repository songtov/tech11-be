#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AXPress Scholar Agent - arXiv API ê¸°ë°˜ ë…¼ë¬¸ ê²€ìƒ‰ ë° ì¶”ì²œ ì‹œìŠ¤í…œ
íŠ¹ì • ë„ë©”ì¸ì˜ ìµœì‹  ë…¼ë¬¸ê³¼ ì¸ê¸° ë…¼ë¬¸ì„ ì¶”ì²œí•˜ê³  PDF ë‹¤ìš´ë¡œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import requests
import json
import os
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Paper:
    """ë…¼ë¬¸ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    title: str
    authors: List[str]
    published_date: str
    updated_date: str
    abstract: str
    categories: List[str]
    pdf_url: str
    arxiv_url: str
    citation_count: int = 0
    relevance_score: float = 0.0

class AXPressScholarAgent:
    """arXiv APIë¥¼ í™œìš©í•œ ë…¼ë¬¸ ê²€ìƒ‰ ë° ì¶”ì²œ ì—ì´ì „íŠ¸"""
    
    def __init__(self, download_dir: str = "downloaded_papers"):
        self.base_url = "http://export.arxiv.org/api/query"
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(exist_ok=True)
        
        # ë„ë©”ì¸ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ë§¤í•‘
        self.domain_keywords = {
            "ì œì¡°": ["manufacturing", "production", "industrial", "factory", "automation", "robotics"],
            "ê¸ˆìœµ": ["finance", "financial", "banking", "fintech", "investment", "trading", "economics"],
            "CLOUD": ["cloud computing", "distributed systems", "microservices", "kubernetes", "container"],
            "í†µì‹ ": ["telecommunications", "communication", "network", "5G", "6G", "wireless"],
            "ìœ í†µ/ë¬¼ë¥˜": ["logistics", "supply chain", "distribution", "retail", "e-commerce", "optimization"],
            "Gen AI": ["artificial intelligence", "machine learning", "deep learning", "LLM", "generative AI", "neural networks"]
        }
        
        # arXiv ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë” ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´)
        self.arxiv_categories = {
            "ì œì¡°": ["cs.RO", "cs.SY", "eess.SY"],  # Robotics, Systems, Control Systems
            "ê¸ˆìœµ": ["q-fin.GN", "q-fin.CP", "econ.GN"],  # General Finance, Computational Finance
            "CLOUD": ["cs.DC", "cs.DS", "cs.SE"],  # Distributed Computing, Data Structures, Software Engineering
            "í†µì‹ ": ["cs.NI", "eess.SP"],  # Networking, Signal Processing
            "ìœ í†µ/ë¬¼ë¥˜": ["cs.AI", "math.OC"],  # Artificial Intelligence, Optimization
            "Gen AI": ["cs.AI", "cs.LG", "cs.CL"]  # AI, Machine Learning, Computation and Language
        }
    
    def fetch_papers(self, domain: str) -> List[Paper]:
        """ì§€ì •ëœ ë„ë©”ì¸ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        logger.info(f"ë„ë©”ì¸ '{domain}'ì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘")
        
        if domain not in self.domain_keywords:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë„ë©”ì¸ì…ë‹ˆë‹¤: {domain}")
        
        papers = []
        
        try:
            # ìµœì‹  ë…¼ë¬¸ 4í¸ ê²€ìƒ‰ (ìµœê·¼ 1ë…„)
            latest_papers = self._search_latest_papers(domain, max_results=10)
            
            # ì¸ê¸° ë…¼ë¬¸ 1í¸ ê²€ìƒ‰ (ì¸ìš©ìˆ˜ ê¸°ì¤€)
            popular_paper = self._search_popular_paper(domain)
            
            # ìµœì‹  ë…¼ë¬¸ 4í¸ ì„ íƒ (ì¤‘ë³µ ì œê±°)
            selected_latest = []
            seen_ids = set()
            
            for paper in latest_papers:
                if paper.id not in seen_ids and len(selected_latest) < 4:
                    selected_latest.append(paper)
                    seen_ids.add(paper.id)
            
            # ì¸ê¸° ë…¼ë¬¸ì´ ì¤‘ë³µë˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
            if popular_paper and popular_paper.id not in seen_ids:
                papers.append(popular_paper)
            elif popular_paper:
                # ì¸ê¸° ë…¼ë¬¸ì´ ì¤‘ë³µì´ë©´ ìµœì‹  ë…¼ë¬¸ ì¤‘ í•˜ë‚˜ë¥¼ ëŒ€ì²´
                if selected_latest:
                    selected_latest[0] = popular_paper
            
            papers.extend(selected_latest)
            
            # 5í¸ì´ ë˜ë„ë¡ ì¡°ì •
            papers = papers[:5]
            
            logger.info(f"ì´ {len(papers)}í¸ì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return papers
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def _search_latest_papers(self, domain: str, max_results: int = 10) -> List[Paper]:
        """ìµœì‹  ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        keywords = self.domain_keywords[domain]
        categories = self.arxiv_categories.get(domain, [])
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = " OR ".join([f'all:{keyword}' for keyword in keywords])
        if categories:
            category_query = " OR ".join([f'cat:{cat}' for cat in categories])
            search_query = f"({search_query}) OR ({category_query})"
        
        # 1ë…„ ì „ ë‚ ì§œ ê³„ì‚°
        one_year_ago = datetime.now() - timedelta(days=365)
        date_filter = one_year_ago.strftime("%Y%m%d")
        
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': max_results * 2,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        return self._make_arxiv_request(params, max_results)
    
    def _search_popular_paper(self, domain: str) -> Optional[Paper]:
        """ì¸ê¸° ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (relevance ê¸°ì¤€)."""
        keywords = self.domain_keywords[domain]
        categories = self.arxiv_categories.get(domain, [])
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = " OR ".join([f'all:{keyword}' for keyword in keywords])
        if categories:
            category_query = " OR ".join([f'cat:{cat}' for cat in categories])
            search_query = f"({search_query}) OR ({category_query})"
        
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': 20,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }
        
        papers = self._make_arxiv_request(params, 1)
        return papers[0] if papers else None
    
    def _make_arxiv_request(self, params: Dict, max_results: int) -> List[Paper]:
        """arXiv APIì— ìš”ì²­ì„ ë³´ë‚´ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"arXiv API ìš”ì²­: {params}")
            
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # XML íŒŒì‹±
            from xml.etree import ElementTree as ET
            root = ET.fromstring(response.content)
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            papers = []
            entries = root.findall('atom:entry', ns)
            
            for entry in entries[:max_results]:
                paper = self._parse_arxiv_entry(entry, ns)
                if paper:
                    papers.append(paper)
            
            return papers
            
        except requests.exceptions.RequestException as e:
            logger.error(f"arXiv API ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def _parse_arxiv_entry(self, entry, ns: Dict) -> Optional[Paper]:
        """arXiv ì—”íŠ¸ë¦¬ë¥¼ íŒŒì‹±í•˜ì—¬ Paper ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # ID ì¶”ì¶œ
            id_elem = entry.find('atom:id', ns)
            paper_id = id_elem.text.strip() if id_elem is not None else "unknown"
            
            # ì œëª© ì¶”ì¶œ
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text.strip() if title_elem is not None else "No Title"
            
            # ì €ì ì¶”ì¶œ
            authors = []
            for author in entry.findall('atom:author', ns):
                name_elem = author.find('atom:name', ns)
                if name_elem is not None:
                    authors.append(name_elem.text.strip())
            
            # ë‚ ì§œ ì¶”ì¶œ
            published_elem = entry.find('atom:published', ns)
            published_date = published_elem.text.strip() if published_elem is not None else ""
            
            updated_elem = entry.find('atom:updated', ns)
            updated_date = updated_elem.text.strip() if updated_elem is not None else ""
            
            # ìš”ì•½ ì¶”ì¶œ
            summary_elem = entry.find('atom:summary', ns)
            abstract = summary_elem.text.strip() if summary_elem is not None else ""
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            categories = []
            for category in entry.findall('atom:category', ns):
                term = category.get('term')
                if term:
                    categories.append(term)
            
            # PDF URL ìƒì„±
            pdf_url = f"https://arxiv.org/pdf/{paper_id.split('/')[-1]}.pdf"
            arxiv_url = paper_id
            
            return Paper(
                id=paper_id,
                title=title,
                authors=authors,
                published_date=published_date,
                updated_date=updated_date,
                abstract=abstract,
                categories=categories,
                pdf_url=pdf_url,
                arxiv_url=arxiv_url
            )
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def display_papers(self, papers: List[Paper]) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        if not papers:
            print("ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*80)
        print("ğŸ“š AXPress Scholar Agent - ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼")
        print("="*80)
        
        for i, paper in enumerate(papers, 1):
            print(f"\n{i}. {paper.title}")
            print(f"   ì €ì: {', '.join(paper.authors[:3])}{' ì™¸' if len(paper.authors) > 3 else ''}")
            
            # ë‚ ì§œ í¬ë§·íŒ…
            try:
                pub_date = datetime.fromisoformat(paper.published_date.replace('Z', '+00:00'))
                formatted_date = pub_date.strftime("%Y-%m-%d")
                print(f"   ë°œí‘œì¼: {formatted_date}")
            except:
                print(f"   ë°œí‘œì¼: {paper.published_date}")
            
            print(f"   ì¹´í…Œê³ ë¦¬: {', '.join(paper.categories[:3])}")
            print(f"   PDF: [PDF Available] - {paper.pdf_url}")
            
            # ìš”ì•½ ë¯¸ë¦¬ë³´ê¸° (ì²« 100ì)
            if paper.abstract:
                preview = paper.abstract[:100] + "..." if len(paper.abstract) > 100 else paper.abstract
                print(f"   ìš”ì•½: {preview}")
    
    def download_pdf(self, paper: Paper) -> Optional[str]:
        """ë…¼ë¬¸ì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ëª… ìƒì„±
            safe_title = "".join(c for c in paper.title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title[:100]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
            filename = f"{safe_title}_{paper.id.split('/')[-1]}.pdf"
            filepath = self.download_dir / filename
            
            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if filepath.exists():
                logger.info(f"íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {filepath}")
                return str(filepath)
            
            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì‹œì‘: {paper.pdf_url}")
            print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {paper.title[:50]}...")
            
            # PDF ë‹¤ìš´ë¡œë“œ
            response = requests.get(paper.pdf_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            print(f"âœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            return str(filepath)
            
        except requests.exceptions.Timeout:
            logger.error("PDF ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            print("âŒ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            return None
        except requests.exceptions.ConnectionError:
            logger.error("PDF ë‹¤ìš´ë¡œë“œ ì—°ê²° ì˜¤ë¥˜")
            print("âŒ ì—°ê²° ì˜¤ë¥˜")
            return None
        except requests.exceptions.HTTPError as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ HTTP ì˜¤ë¥˜: {e}")
            print(f"âŒ HTTP ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”¬ AXPress Scholar Agent - arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("="*60)
    
    # ì§€ì› ë„ë©”ì¸ í‘œì‹œ
    domains = ["ì œì¡°", "ê¸ˆìœµ", "CLOUD", "í†µì‹ ", "ìœ í†µ/ë¬¼ë¥˜", "Gen AI"]
    print("\nğŸ“‹ ì§€ì› ë„ë©”ì¸:")
    for i, domain in enumerate(domains, 1):
        print(f"   {i}. {domain}")
    
    try:
        # ë„ë©”ì¸ ì„ íƒ
        while True:
            try:
                choice = input(f"\në„ë©”ì¸ì„ ì„ íƒí•˜ì„¸ìš” (1-{len(domains)}): ").strip()
                domain_index = int(choice) - 1
                
                if 0 <= domain_index < len(domains):
                    selected_domain = domains[domain_index]
                    break
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì—ì´ì „íŠ¸ ìƒì„± ë° ë…¼ë¬¸ ê²€ìƒ‰
        agent = AXPressScholarAgent()
        print(f"\nğŸ” '{selected_domain}' ë„ë©”ì¸ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        
        papers = agent.fetch_papers(selected_domain)
        
        if not papers:
            print("ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê²°ê³¼ í‘œì‹œ
        agent.display_papers(papers)
        
        # PDF ë‹¤ìš´ë¡œë“œ ì„ íƒ
        print(f"\nğŸ“„ ì´ {len(papers)}í¸ì˜ ë…¼ë¬¸ ì¤‘ì—ì„œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•  ë…¼ë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.")
        print("ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (0: ì¢…ë£Œ): ", end="")
        
        try:
            choice = int(input().strip())
            if choice == 0:
                print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
            elif 1 <= choice <= len(papers):
                selected_paper = papers[choice - 1]
                print(f"\nì„ íƒëœ ë…¼ë¬¸: {selected_paper.title}")
                
                filepath = agent.download_pdf(selected_paper)
                
                if filepath:
                    print(f"\nâœ… PDF ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    print(f"ì €ì¥ ìœ„ì¹˜: {filepath}")
                else:
                    print("\nâŒ PDF ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main()
