#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FastAPI Framework - AI Boot Camp Lab Final Version 1
ë…¼ë¬¸ ê²€ìƒ‰ â†’ PDF ë‹¤ìš´ë¡œë“œ â†’ ë©€í‹°ì—ì´ì „íŠ¸ ê¸°ë°˜ í€´ì¦ˆ/ìš”ì•½/í•´ì„¤ ìƒì„± ë° TTS íŒŸìºìŠ¤íŠ¸ ì œì‘ì„ ìë™í™”í•˜ëŠ” REST API
multitest.pyì˜ ê³ ë„í™”ëœ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œê³¼ axpress_scholar_agent_ver1.pyë¥¼ ê²°í•©
"""

import os
import re
import tempfile
import requests
import json
import sys
import subprocess
import time
import asyncio
from pathlib import Path
from typing import List, Optional, Dict, Any, TypedDict
from datetime import datetime, timedelta
from dataclasses import dataclass
import logging

# FastAPI ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ========== LangChain / LangGraph ==========
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# LangGraph
from langgraph.graph import StateGraph, END

# TTS
from gtts import gTTS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==========================
# ë°ì´í„° í´ë˜ìŠ¤ ë° ëª¨ë¸ë“¤
# ==========================
@dataclass
class Paper:
    """ë…¼ë¬¸ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    title: str
    authors: List[str]
    published_date: str
    updated_date: str
    abstract: str
    categories: List[str]
    pdf_url: str
    arxiv_url: str
    citation_count: int = 0
    relevance_score: float = 0.0

# Pydantic ëª¨ë¸ë“¤
class DomainRequest(BaseModel):
    domain: str = Field(..., description="ê²€ìƒ‰í•  ë„ë©”ì¸ (ì œì¡°, ê¸ˆìœµ, CLOUD, í†µì‹ , ìœ í†µ/ë¬¼ë¥˜, Gen AI)")
    additional_keywords: Optional[str] = Field(None, description="ì¶”ê°€ ê²€ìƒ‰ í‚¤ì›Œë“œ")
    year_range: int = Field(1, ge=1, le=2, description="ê²€ìƒ‰ ê¸°ê°„ (1-2ë…„)")

class PaperResponse(BaseModel):
    id: str
    title: str
    authors: List[str]
    published_date: str
    updated_date: str
    abstract: str
    categories: List[str]
    pdf_url: str
    arxiv_url: str
    citation_count: int = 0
    relevance_score: float = 0.0

class DownloadRequest(BaseModel):
    paper_index: int = Field(..., ge=0, description="ë‹¤ìš´ë¡œë“œí•  ë…¼ë¬¸ ì¸ë±ìŠ¤")

class QuizRequest(BaseModel):
    pdf_path: str = Field(..., description="ë¶„ì„í•  PDF íŒŒì¼ ê²½ë¡œ")

class WorkflowRequest(BaseModel):
    domain: str = Field(..., description="ê²€ìƒ‰í•  ë„ë©”ì¸")
    additional_keywords: Optional[str] = Field(None, description="ì¶”ê°€ ê²€ìƒ‰ í‚¤ì›Œë“œ")
    year_range: int = Field(1, ge=1, le=2, description="ê²€ìƒ‰ ê¸°ê°„")
    paper_index: int = Field(0, ge=0, description="ë‹¤ìš´ë¡œë“œí•  ë…¼ë¬¸ ì¸ë±ìŠ¤")

class WorkflowResponse(BaseModel):
    success: bool
    message: str
    workflow_id: str
    papers: List[PaperResponse]
    downloaded_pdf: Optional[str] = None
    quiz_generated: bool = False
    summary_generated: bool = False
    explainer_generated: bool = False
    podcast_created: bool = False

class StatusResponse(BaseModel):
    workflow_id: str
    status: str
    progress: int
    current_step: str
    message: str
    results: Optional[Dict[str, Any]] = None

class MultiAgentResponse(BaseModel):
    success: bool
    message: str
    summary: Optional[str] = None
    quiz: Optional[str] = None
    explainer: Optional[str] = None
    figure_analysis: Optional[str] = None
    tts_file: Optional[str] = None

# ==========================
# LangGraph State ì •ì˜
# ==========================
class AgentState(TypedDict, total=False):
    # ì…ë ¥/ê³µìœ 
    vectorstore: Any
    query_summary: str
    query_quiz: str
    query_explainer: str
    query_figure_analysis: str
    k: int  # RAG ê²€ìƒ‰ ê°œìˆ˜

    # ì‚°ì¶œë¬¼
    summary: str
    quiz: str
    explainer: str
    figure_analysis: str

    # ë‚´ë¶€ ì‹ í˜¸
    judge_summary_ok: bool
    judge_quiz_ok: bool
    judge_explainer_ok: bool
    judge_figure_analysis_ok: bool

# ==========================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==========================
def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬(TTSìš©)"""
    cleaned = re.sub(r"[#*>â€¢\-]+", " ", text)
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()

def build_llm(use_mini: bool = True, temperature: float = 0.2):
    """LLM íŒ©í† ë¦¬"""
    return AzureChatOpenAI(
        openai_api_version="2024-02-01",
        azure_deployment=os.getenv("AOAI_DEPLOY_GPT4O_MINI") if use_mini else os.getenv("AOAI_DEPLOY_GPT4O"),
        api_key=os.getenv("AOAI_API_KEY"),
        azure_endpoint=os.getenv("AOAI_ENDPOINT"),
        temperature=temperature,
    )

def build_embeddings():
    """Embeddings íŒ©í† ë¦¬"""
    return AzureOpenAIEmbeddings(
        model=os.getenv("AOAI_DEPLOY_EMBED_3_LARGE"),
        openai_api_version="2024-02-01",
        api_key=os.getenv("AOAI_API_KEY"),
        azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    )

# ==========================
# PDF ë¡œë“œ & ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•
# ==========================
def load_pdf(path_or_url: str) -> List[Document]:
    """PDF ë¡œë“œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” URL)"""
    if path_or_url.startswith("http://") or path_or_url.startswith("https://"):
        resp = requests.get(path_or_url, timeout=30)
        resp.raise_for_status()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(resp.content)
            tmp_path = tmp.name
        # íŒŒì¼ í•¸ë“¤ ë‹«íŒ ë’¤ ë¡œë”ê°€ ì—´ë„ë¡
        loader = PyMuPDFLoader(tmp_path)
        docs = loader.load()
        os.unlink(tmp_path)
    else:
        loader = PyMuPDFLoader(path_or_url)
        docs = loader.load()
    return docs

def build_vectorstore(docs: List[Document], embeddings, chunk_size=1000, chunk_overlap=200):
    """ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•"""
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    splits = splitter.split_documents(docs)
    for d in splits:
        page = d.metadata.get("page", None)
        src = d.metadata.get("source", "")
        prefix = f"[source: {os.path.basename(src)} | page: {page}] "
        d.page_content = prefix + d.page_content
    vs = FAISS.from_documents(splits, embeddings)
    return vs

# ==========================
# ì²´ì¸: ìš”ì•½ / í€´ì¦ˆ / í•´ì„¤ / íŒì •
# ==========================
def make_summary_chain():
    """ìš”ì•½ ìƒì„± ì²´ì¸"""
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë…¼ë¬¸ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë¬¸ì„œë¥¼ ì½ê³  ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ ê°„ê²°í•˜ê³  êµ¬ì¡°í™”ëœ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.

1) í•œ ì¤„ ìš”ì•½
2) ì—°êµ¬ ë°°ê²½ê³¼ ë¬¸ì œ ì •ì˜
3) í•µì‹¬ ê¸°ìˆ ê³¼ ë°©ë²•ë¡ 
4) ì£¼ìš” ê²°ê³¼ì™€ ì„±ëŠ¥
5) ê¸°ìˆ ì  ì‹œì‚¬ì ê³¼ í•œê³„
6) í•µì‹¬ í‚¤ì›Œë“œ

ë¬¸ì„œ ë‚´ìš©:
{document_content}
"""
    )
    return prompt | build_llm(use_mini=True) | StrOutputParser()

def make_quiz_chain():
    """í€´ì¦ˆ ìƒì„± ì²´ì¸"""
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë…¼ë¬¸ ê¸°ë°˜ í€´ì¦ˆ ì œì‘ìì…ë‹ˆë‹¤. ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í•œêµ­ì–´ í€´ì¦ˆë¥¼ ì‘ì„±í•˜ì„¸ìš”.

- ì´ 5ë¬¸í•­: ê°ê´€ì‹/ì£¼ê´€ì‹/ì„œìˆ í˜• í˜¼í•©
- ê° ë¬¸í•­ë§ˆë‹¤ ì •ë‹µê³¼ í•´ì„¤ í¬í•¨
- ì €ì/ì—°ë„ ê°™ì€ ì§€ì—½ì  ì‚¬ì‹¤ì€ í”¼í•˜ê³ , í•µì‹¬ ê°œë… ì¤‘ì‹¬
- ë§ˆì§€ë§‰ì— 'ìƒê°í•´ë³¼ ì˜ê²¬ 3ê°€ì§€'ì™€ 'ì‹¤ë¬´ ì ìš© ë°©í–¥ 3ê°€ì§€' ì¶”ê°€

ë¬¸ì„œ ë‚´ìš©:
{document_content}
"""
    )
    return prompt | build_llm(use_mini=True) | StrOutputParser()

def make_explainer_chain():
    """í•´ì„¤ ìƒì„± ì²´ì¸"""
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì „ë¬¸ í•´ì„¤ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ í•´ì„¤ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

êµ¬ì„±:
1) ë…¼ë¬¸ì˜ ìƒì„¸ ì„¤ëª…
2) ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ ì„¤ëª…
3) ì‚°ì—… í˜„ì¥ì—ì„œì˜ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ 2~3ê°€ì§€

ë¬¸ì„œ ë‚´ìš©:
{document_content}
"""
    )
    return prompt | build_llm(use_mini=False) | StrOutputParser()

def make_figure_analysis_chain():
    """ê·¸ë¦¼ ë¶„ì„ ì²´ì¸"""
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë…¼ë¬¸ì˜ ê·¸ë¦¼, ì°¨íŠ¸, ê·¸ë˜í”„ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë¬¸ì„œë¥¼ ì½ê³  ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ ìƒì„¸í•œ ê·¸ë¦¼ ë¶„ì„ì„ ì‘ì„±í•˜ì„¸ìš”.

1) ì£¼ìš” ê·¸ë¦¼/ì°¨íŠ¸/ê·¸ë˜í”„ ì‹ë³„
2) ê° ê·¸ë¦¼ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ì˜ë¯¸
3) ë°ì´í„° í•´ì„ ë° ì¸ì‚¬ì´íŠ¸
4) ê·¸ë¦¼ ê°„ì˜ ì—°ê´€ì„±ê³¼ íë¦„
5) ì‹¤ë¬´ ì ìš© ì‹œ ì‹œê°í™” ë°©í–¥

ë¬¸ì„œ ë‚´ìš©:
{document_content}
"""
    )
    return prompt | build_llm(use_mini=False) | StrOutputParser()


def make_judge_chain():
    """í’ˆì§ˆ íŒì • ì²´ì¸"""
    prompt = PromptTemplate.from_template(
        """ë‹¤ìŒ ìƒì„±ë¬¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.
ê¸°ì¤€: (A) ë¬¸ì„œ í•µì‹¬ ì£¼ì œë¥¼ ë¹ ì§ì—†ì´ ë‹¤ë£¨ì—ˆëŠ”ê°€, (B) ë…¼ë¦¬ì  ì¼ê´€ì„±, (C) ê³¼ë„í•œ í™˜ê°(ë¬¸ì„œì— ì—†ëŠ” ì£¼ì¥) ì—¬ë¶€.

ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥:
- YES: ì¶©ë¶„íˆ ì–‘í˜¸
- NO: ë¶ˆì¶©ë¶„ (ì¬ê²€ìƒ‰/ì¬ìƒì„± í•„ìš”)

í‰ê°€í•  í…ìŠ¤íŠ¸:
{generated}
"""
    )
    # íŒë‹¨ì€ ë³´ìˆ˜ì ìœ¼ë¡œ â†’ gpt-4oë¡œ íŒë‹¨
    return prompt | build_llm(use_mini=False, temperature=0.0) | StrOutputParser()

# ==========================
# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
# ==========================
def node_summarizer(state: AgentState) -> AgentState:
    """ìš”ì•½ ìƒì„± ë…¸ë“œ"""
    vs = state["vectorstore"]
    k = state.get("k", 12)
    chunks = vs.similarity_search(state.get("query_summary", "summary overview of this document"), k=k)
    content = "\n\n".join([c.page_content for c in chunks])
    summary_chain = make_summary_chain()
    summary = summary_chain.invoke({"document_content": content})
    return {"summary": summary}

def node_quiz(state: AgentState) -> AgentState:
    """í€´ì¦ˆ ìƒì„± ë…¸ë“œ"""
    vs = state["vectorstore"]
    k = state.get("k", 10)
    chunks = vs.similarity_search(state.get("query_quiz", "Generate exam questions based on this document"), k=k)
    content = "\n\n".join([c.page_content for c in chunks])
    quiz_chain = make_quiz_chain()
    quiz = quiz_chain.invoke({"document_content": content})
    return {"quiz": quiz}

def node_explainer(state: AgentState) -> AgentState:
    """í•´ì„¤ ìƒì„± ë…¸ë“œ"""
    vs = state["vectorstore"]
    k = state.get("k", 15)
    chunks = vs.similarity_search(state.get("query_explainer", "detailed explanation with industry applications"), k=k)
    content = "\n\n".join([c.page_content for c in chunks])
    explainer_chain = make_explainer_chain()
    explainer = explainer_chain.invoke({"document_content": content})
    return {"explainer": explainer}

def node_judge_summary(state: AgentState) -> AgentState:
    """ìš”ì•½ í’ˆì§ˆ íŒì • ë…¸ë“œ"""
    judge = make_judge_chain()
    verdict = judge.invoke({"generated": state.get("summary", "")}).strip().upper()
    return {"judge_summary_ok": verdict.startswith("YES")}

def node_judge_quiz(state: AgentState) -> AgentState:
    """í€´ì¦ˆ í’ˆì§ˆ íŒì • ë…¸ë“œ"""
    judge = make_judge_chain()
    verdict = judge.invoke({"generated": state.get("quiz", "")}).strip().upper()
    return {"judge_quiz_ok": verdict.startswith("YES")}

def node_judge_explainer(state: AgentState) -> AgentState:
    """í•´ì„¤ í’ˆì§ˆ íŒì • ë…¸ë“œ"""
    judge = make_judge_chain()
    verdict = judge.invoke({"generated": state.get("explainer", "")}).strip().upper()
    return {"judge_explainer_ok": verdict.startswith("YES")}

def cond_on_summary(state: AgentState) -> str:
    """ìš”ì•½ ì¡°ê±´ë¶€ ì—£ì§€ ë¼ìš°íŒ…"""
    if state.get("judge_summary_ok", True):
        return "ok"
    # ë¯¸í¡í•˜ë©´ k + 4 (ìƒí•œì„  40)
    new_k = min(40, state.get("k", 12) + 4)
    state["k"] = new_k
    return "retry"

def cond_on_quiz(state: AgentState) -> str:
    """í€´ì¦ˆ ì¡°ê±´ë¶€ ì—£ì§€ ë¼ìš°íŒ…"""
    if state.get("judge_quiz_ok", True):
        return "ok"
    new_k = min(40, state.get("k", 12) + 4)
    state["k"] = new_k
    return "retry"

def cond_on_explainer(state: AgentState) -> str:
    """í•´ì„¤ ì¡°ê±´ë¶€ ì—£ì§€ ë¼ìš°íŒ…"""
    if state.get("judge_explainer_ok", True):
        return "ok"
    new_k = min(40, state.get("k", 12) + 4)
    state["k"] = new_k
    return "retry"

def node_figure_analysis(state: AgentState) -> AgentState:
    """ê·¸ë¦¼ ë¶„ì„ ë…¸ë“œ"""
    vs = state["vectorstore"]
    k = state.get("k", 12)
    chunks = vs.similarity_search(
        state.get("query_figure_analysis", "figure analysis and visualization interpretation"), k=k
    )
    content = "\n\n".join([c.page_content for c in chunks])
    figure_analysis_chain = make_figure_analysis_chain()
    figure_analysis = figure_analysis_chain.invoke({"document_content": content})
    return {"figure_analysis": figure_analysis}


def node_judge_figure_analysis(state: AgentState) -> AgentState:
    """ê·¸ë¦¼ ë¶„ì„ í’ˆì§ˆ íŒì • ë…¸ë“œ"""
    judge = make_judge_chain()
    verdict = judge.invoke({"generated": state.get("figure_analysis", "")}).strip().upper()
    return {"judge_figure_analysis_ok": verdict.startswith("YES")}


def cond_on_figure_analysis(state: AgentState) -> str:
    """ê·¸ë¦¼ ë¶„ì„ ì¡°ê±´ë¶€ ì—£ì§€ ë¼ìš°íŒ…"""
    if state.get("judge_figure_analysis_ok", True):
        return "ok"
    new_k = min(40, state.get("k", 12) + 4)
    state["k"] = new_k
    return "retry"


def node_tts(state: AgentState) -> AgentState:
    """TTS ìƒì„± ë…¸ë“œ"""
    script = state.get("explainer", "")
    if not script:
        return {}
    script_clean = clean_text(script)
    tts = gTTS(text=script_clean, lang="ko")
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_mp3 = f"industry_explainer_{ts}.mp3"
    tts.save(out_mp3)
    print(f"ğŸ§ TTS ì €ì¥ ì™„ë£Œ: {out_mp3}")
    return {}

# ==========================
# ì›Œí¬í”Œë¡œìš° êµ¬ì„± (LangGraph)
# ==========================
def build_workflow():
    """ë©€í‹°ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
    graph = StateGraph(AgentState)

    # ë…¸ë“œ ë“±ë¡
    graph.add_node("summarizer", node_summarizer)
    graph.add_node("judge_summary", node_judge_summary)
    graph.add_node("quiz", node_quiz)
    graph.add_node("judge_quiz", node_judge_quiz)
    graph.add_node("explainer", node_explainer)
    graph.add_node("judge_explainer", node_judge_explainer)
    graph.add_node("figure_analysis", node_figure_analysis)
    graph.add_node("judge_figure_analysis", node_judge_figure_analysis)
    graph.add_node("tts", node_tts)

    # ì§„ì…ì 
    graph.set_entry_point("summarizer")

    # summarizer â†’ judge â†’ (retry: summarizer, ok: quiz)
    graph.add_edge("summarizer", "judge_summary")
    graph.add_conditional_edges(
        "judge_summary",
        cond_on_summary,
        {
            "retry": "summarizer",
            "ok": "quiz"
        }
    )

    # quiz â†’ judge â†’ (retry: quiz, ok: explainer)
    graph.add_edge("quiz", "judge_quiz")
    graph.add_conditional_edges(
        "judge_quiz",
        cond_on_quiz,
        {
            "retry": "quiz",
            "ok": "explainer"
        }
    )

    # explainer â†’ judge â†’ (retry: explainer, ok: figure_analysis)
    graph.add_edge("explainer", "judge_explainer")
    graph.add_conditional_edges(
        "judge_explainer",
        cond_on_explainer,
        {
            "retry": "explainer",
            "ok": "figure_analysis"
        }
    )

    # figure_analysis â†’ judge â†’ (retry: figure_analysis, ok: tts)
    graph.add_edge("figure_analysis", "judge_figure_analysis")
    graph.add_conditional_edges(
        "judge_figure_analysis",
        cond_on_figure_analysis,
        {
            "retry": "figure_analysis",
            "ok": "tts"
        }
    )

    # ë§ˆì§€ë§‰ tts í›„ ì¢…ë£Œ
    graph.add_edge("tts", END)

    return graph.compile()

# ==========================
# ë©€í‹°ì—ì´ì „íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
# ==========================
def run_multi_agent(pdf_path_or_url: str):
    """ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    print(f"\nğŸ¯ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path_or_url}")
    docs = load_pdf(pdf_path_or_url)
    print(f"âœ… PDF ë¡œë“œ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ")

    embeddings = build_embeddings()
    vs = build_vectorstore(docs, embeddings)
    print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ")

    workflow = build_workflow()
    init_state: AgentState = {
        "vectorstore": vs,
        "k": 12,
        "query_summary": "summary overview of this document",
        "query_quiz": "Generate exam questions based on this document",
        "query_explainer": "detailed explanation with industry applications",
        "query_figure_analysis": "figure analysis and visualization interpretation",
    }

    # ì‹¤í–‰
    final_state = workflow.invoke(init_state)

    # ì‚°ì¶œë¬¼ ì €ì¥
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    results = {}
    
    if final_state.get("summary"):
        with open(f"summary_{ts}.txt", "w", encoding="utf-8") as f:
            f.write(final_state["summary"])
        print(f"ğŸ“ ìš”ì•½ ì €ì¥: summary_{ts}.txt")
        results["summary"] = final_state["summary"]

    if final_state.get("quiz"):
        with open(f"quiz_{ts}.txt", "w", encoding="utf-8") as f:
            f.write(final_state["quiz"])
        print(f"ğŸ“ í€´ì¦ˆ ì €ì¥: quiz_{ts}.txt")
        results["quiz"] = final_state["quiz"]

    if final_state.get("explainer"):
        with open(f"explainer_{ts}.txt", "w", encoding="utf-8") as f:
            f.write(final_state["explainer"])
        print(f"ğŸ“ í•´ì„¤ ì €ì¥: explainer_{ts}.txt")
        results["explainer"] = final_state["explainer"]

    if final_state.get("figure_analysis"):
        with open(f"figure_analysis_{ts}.txt", "w", encoding="utf-8") as f:
            f.write(final_state["figure_analysis"])
        print(f"ğŸ“Š ê·¸ë¦¼ ë¶„ì„ ì €ì¥: figure_analysis_{ts}.txt")
        results["figure_analysis"] = final_state["figure_analysis"]

    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    return results

# ==========================
# AXPressScholarAgent í´ë˜ìŠ¤
# ==========================
class AXPressScholarAgent:
    """arXiv APIë¥¼ í™œìš©í•œ ë…¼ë¬¸ ê²€ìƒ‰ ë° ì¶”ì²œ ì—ì´ì „íŠ¸"""
    
    def __init__(self, download_dir: str = "downloaded_papers"):
        self.base_url = "http://export.arxiv.org/api/query"
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(exist_ok=True)
        
        # ë„ë©”ì¸ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ë§¤í•‘
        self.domain_keywords = {
            "ì œì¡°": ["manufacturing", "production", "industrial", "factory", "automation", "robotics"],
            "ê¸ˆìœµ": ["finance", "financial", "banking", "fintech", "investment", "trading", "economics"],
            "CLOUD": ["cloud computing", "distributed systems", "microservices", "kubernetes", "container"],
            "í†µì‹ ": ["telecommunications", "communication", "network", "5G", "6G", "wireless"],
            "ìœ í†µ/ë¬¼ë¥˜": ["logistics", "supply chain", "distribution", "retail", "e-commerce", "optimization"],
            "Gen AI": ["artificial intelligence", "machine learning", "deep learning", "LLM", "generative AI", "neural networks"]
        }
        
        # arXiv ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë” ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´)
        self.arxiv_categories = {
            "ì œì¡°": ["cs.RO", "cs.SY", "eess.SY"],  # Robotics, Systems, Control Systems
            "ê¸ˆìœµ": ["q-fin.GN", "q-fin.CP", "econ.GN"],  # General Finance, Computational Finance
            "CLOUD": ["cs.DC", "cs.DS", "cs.SE"],  # Distributed Computing, Data Structures, Software Engineering
            "í†µì‹ ": ["cs.NI", "eess.SP"],  # Networking, Signal Processing
            "ìœ í†µ/ë¬¼ë¥˜": ["cs.AI", "math.OC"],  # Artificial Intelligence, Optimization
            "Gen AI": ["cs.AI", "cs.LG", "cs.CL"]  # AI, Machine Learning, Computation and Language
        }
    
    def fetch_papers(self, domain: str) -> List[Paper]:
        """ì§€ì •ëœ ë„ë©”ì¸ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        logger.info(f"ë„ë©”ì¸ '{domain}'ì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘")
        
        if domain not in self.domain_keywords:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë„ë©”ì¸ì…ë‹ˆë‹¤: {domain}")
        
        papers = []
        
        try:
            # ìµœì‹  ë…¼ë¬¸ 4í¸ ê²€ìƒ‰ (ìµœê·¼ 1ë…„)
            latest_papers = self._search_latest_papers(domain, max_results=10)
            
            # ì¸ê¸° ë…¼ë¬¸ 1í¸ ê²€ìƒ‰ (ì¸ìš©ìˆ˜ ê¸°ì¤€)
            popular_paper = self._search_popular_paper(domain)
            
            # ìµœì‹  ë…¼ë¬¸ 4í¸ ì„ íƒ (ì¤‘ë³µ ì œê±°)
            selected_latest = []
            seen_ids = set()
            
            for paper in latest_papers:
                if paper.id not in seen_ids and len(selected_latest) < 4:
                    selected_latest.append(paper)
                    seen_ids.add(paper.id)
            
            # ì¸ê¸° ë…¼ë¬¸ì´ ì¤‘ë³µë˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
            if popular_paper and popular_paper.id not in seen_ids:
                papers.append(popular_paper)
            elif popular_paper:
                # ì¸ê¸° ë…¼ë¬¸ì´ ì¤‘ë³µì´ë©´ ìµœì‹  ë…¼ë¬¸ ì¤‘ í•˜ë‚˜ë¥¼ ëŒ€ì²´
                if selected_latest:
                    selected_latest[0] = popular_paper
            
            papers.extend(selected_latest)
            
            # 5í¸ì´ ë˜ë„ë¡ ì¡°ì •
            papers = papers[:5]
            
            logger.info(f"ì´ {len(papers)}í¸ì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return papers
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def _search_latest_papers(self, domain: str, max_results: int = 10) -> List[Paper]:
        """ìµœì‹  ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        keywords = self.domain_keywords[domain]
        categories = self.arxiv_categories.get(domain, [])
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = " OR ".join([f'all:{keyword}' for keyword in keywords])
        if categories:
            category_query = " OR ".join([f'cat:{cat}' for cat in categories])
            search_query = f"({search_query}) OR ({category_query})"
        
        # 1ë…„ ì „ ë‚ ì§œ ê³„ì‚°
        one_year_ago = datetime.now() - timedelta(days=365)
        date_filter = one_year_ago.strftime("%Y%m%d")
        
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': max_results * 2,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        return self._make_arxiv_request(params, max_results)
    
    def _search_popular_paper(self, domain: str) -> Optional[Paper]:
        """ì¸ê¸° ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (relevance ê¸°ì¤€)."""
        keywords = self.domain_keywords[domain]
        categories = self.arxiv_categories.get(domain, [])
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = " OR ".join([f'all:{keyword}' for keyword in keywords])
        if categories:
            category_query = " OR ".join([f'cat:{cat}' for cat in categories])
            search_query = f"({search_query}) OR ({category_query})"
        
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': 20,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }
        
        papers = self._make_arxiv_request(params, 1)
        return papers[0] if papers else None
    
    def _make_arxiv_request(self, params: Dict, max_results: int) -> List[Paper]:
        """arXiv APIì— ìš”ì²­ì„ ë³´ë‚´ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"arXiv API ìš”ì²­: {params}")
            
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # XML íŒŒì‹±
            from xml.etree import ElementTree as ET
            root = ET.fromstring(response.content)
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            papers = []
            entries = root.findall('atom:entry', ns)
            
            for entry in entries[:max_results]:
                paper = self._parse_arxiv_entry(entry, ns)
                if paper:
                    papers.append(paper)
            
            return papers
            
        except requests.exceptions.RequestException as e:
            logger.error(f"arXiv API ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def _parse_arxiv_entry(self, entry, ns: Dict) -> Optional[Paper]:
        """arXiv ì—”íŠ¸ë¦¬ë¥¼ íŒŒì‹±í•˜ì—¬ Paper ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # ID ì¶”ì¶œ
            id_elem = entry.find('atom:id', ns)
            paper_id = id_elem.text.strip() if id_elem is not None else "unknown"
            
            # ì œëª© ì¶”ì¶œ
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text.strip() if title_elem is not None else "No Title"
            
            # ì €ì ì¶”ì¶œ
            authors = []
            for author in entry.findall('atom:author', ns):
                name_elem = author.find('atom:name', ns)
                if name_elem is not None:
                    authors.append(name_elem.text.strip())
            
            # ë‚ ì§œ ì¶”ì¶œ
            published_elem = entry.find('atom:published', ns)
            published_date = published_elem.text.strip() if published_elem is not None else ""
            
            updated_elem = entry.find('atom:updated', ns)
            updated_date = updated_elem.text.strip() if updated_elem is not None else ""
            
            # ìš”ì•½ ì¶”ì¶œ
            summary_elem = entry.find('atom:summary', ns)
            abstract = summary_elem.text.strip() if summary_elem is not None else ""
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            categories = []
            for category in entry.findall('atom:category', ns):
                term = category.get('term')
                if term:
                    categories.append(term)
            
            # PDF URL ìƒì„±
            pdf_url = f"https://arxiv.org/pdf/{paper_id.split('/')[-1]}.pdf"
            arxiv_url = paper_id
            
            return Paper(
                id=paper_id,
                title=title,
                authors=authors,
                published_date=published_date,
                updated_date=updated_date,
                abstract=abstract,
                categories=categories,
                pdf_url=pdf_url,
                arxiv_url=arxiv_url
            )
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def display_papers(self, papers: List[Paper]) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        if not papers:
            print("ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*80)
        print("ğŸ“š AXPress Scholar Agent - ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼")
        print("="*80)
        
        for i, paper in enumerate(papers, 1):
            print(f"\n{i}. {paper.title}")
            print(f"   ì €ì: {', '.join(paper.authors[:3])}{' ì™¸' if len(paper.authors) > 3 else ''}")
            
            # ë‚ ì§œ í¬ë§·íŒ…
            try:
                pub_date = datetime.fromisoformat(paper.published_date.replace('Z', '+00:00'))
                formatted_date = pub_date.strftime("%Y-%m-%d")
                print(f"   ë°œí‘œì¼: {formatted_date}")
            except:
                print(f"   ë°œí‘œì¼: {paper.published_date}")
            
            print(f"   ì¹´í…Œê³ ë¦¬: {', '.join(paper.categories[:3])}")
            print(f"   PDF: [PDF Available] - {paper.pdf_url}")
            
            # ìš”ì•½ ë¯¸ë¦¬ë³´ê¸° (ì²« 100ì)
            if paper.abstract:
                preview = paper.abstract[:100] + "..." if len(paper.abstract) > 100 else paper.abstract
                print(f"   ìš”ì•½: {preview}")
    
    def download_pdf(self, paper: Paper) -> Optional[str]:
        """ë…¼ë¬¸ì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ëª… ìƒì„±
            safe_title = "".join(c for c in paper.title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title[:100]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
            filename = f"{safe_title}_{paper.id.split('/')[-1]}.pdf"
            filepath = self.download_dir / filename
            
            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if filepath.exists():
                logger.info(f"íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {filepath}")
                return str(filepath)
            
            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì‹œì‘: {paper.pdf_url}")
            print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {paper.title[:50]}...")
            
            # PDF ë‹¤ìš´ë¡œë“œ
            response = requests.get(paper.pdf_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            print(f"âœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            return str(filepath)
            
        except requests.exceptions.Timeout:
            logger.error("PDF ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            print("âŒ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            return None
        except requests.exceptions.ConnectionError:
            logger.error("PDF ë‹¤ìš´ë¡œë“œ ì—°ê²° ì˜¤ë¥˜")
            print("âŒ ì—°ê²° ì˜¤ë¥˜")
            return None
        except requests.exceptions.HTTPError as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ HTTP ì˜¤ë¥˜: {e}")
            print(f"âŒ HTTP ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

# ==========================
# FastAPI ì•± ì´ˆê¸°í™”
# ==========================
# ì‹¤ì œ ëª¨ë¸ë“¤ ì´ˆê¸°í™”
scholar_agent = AXPressScholarAgent()
downloaded_papers_dir = Path("downloaded_papers")
workflow_status = {}  # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¶”ì 

# ì§€ì› ë„ë©”ì¸
SUPPORTED_DOMAINS = ["ì œì¡°", "ê¸ˆìœµ", "CLOUD", "í†µì‹ ", "ìœ í†µ/ë¬¼ë¥˜", "Gen AI"]

from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ ì‹¤í–‰
    downloaded_papers_dir.mkdir(exist_ok=True)
    logger.info("FastAPI ì•±ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì‹¤í–‰
    logger.info("FastAPI ì•±ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# FastAPI ì•±ì— lifespan ì¶”ê°€
app = FastAPI(
    title="AI Boot Camp Lab - Final Version 1 API",
    description="ë…¼ë¬¸ ê²€ìƒ‰ë¶€í„° ë©€í‹°ì—ì´ì „íŠ¸ ê¸°ë°˜ í€´ì¦ˆ/ìš”ì•½/í•´ì„¤ ìƒì„± ë° TTS íŒŸìºìŠ¤íŠ¸ ì œì‘ê¹Œì§€ ìë™í™”í•˜ëŠ” REST API",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================
# FastAPI ì—”ë“œí¬ì¸íŠ¸ë“¤
# ==========================
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "AI Boot Camp Lab - Final Version 1 API",
        "version": "1.0.0",
        "docs": "/docs",
        "supported_domains": SUPPORTED_DOMAINS,
        "features": [
            "ë…¼ë¬¸ ê²€ìƒ‰ (arXiv API)",
            "PDF ë‹¤ìš´ë¡œë“œ",
            "ë©€í‹°ì—ì´ì „íŠ¸ ê¸°ë°˜ ìš”ì•½/í€´ì¦ˆ/í•´ì„¤ ìƒì„±",
            "í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ",
            "TTS íŒŸìºìŠ¤íŠ¸ ìƒì„±"
        ]
    }

@app.get("/domains")
async def get_supported_domains():
    """ì§€ì›ë˜ëŠ” ë„ë©”ì¸ ëª©ë¡ ë°˜í™˜"""
    return {
        "domains": SUPPORTED_DOMAINS,
        "count": len(SUPPORTED_DOMAINS)
    }

@app.post("/search/papers", response_model=List[PaperResponse])
async def search_papers(request: DomainRequest):
    """ë„ë©”ì¸ë³„ ë…¼ë¬¸ ê²€ìƒ‰"""
    try:
        if request.domain not in SUPPORTED_DOMAINS:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë„ë©”ì¸ì…ë‹ˆë‹¤. ì§€ì› ë„ë©”ì¸: {SUPPORTED_DOMAINS}"
            )
        
        logger.info(f"ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: ë„ë©”ì¸={request.domain}, í‚¤ì›Œë“œ={request.additional_keywords}")
        
        # ë…¼ë¬¸ ê²€ìƒ‰ (ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©)
        papers = scholar_agent.fetch_papers(request.domain)
        
        if not papers:
            raise HTTPException(status_code=404, detail="ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # Paper ê°ì²´ë¥¼ PaperResponseë¡œ ë³€í™˜
        paper_responses = []
        for paper in papers:
            paper_responses.append(PaperResponse(
                id=paper.id,
                title=paper.title,
                authors=paper.authors,
                published_date=paper.published_date,
                updated_date=paper.updated_date,
                abstract=paper.abstract,
                categories=paper.categories,
                pdf_url=paper.pdf_url,
                arxiv_url=paper.arxiv_url,
                citation_count=paper.citation_count,
                relevance_score=paper.relevance_score
            ))
        
        logger.info(f"ë…¼ë¬¸ ê²€ìƒ‰ ì™„ë£Œ: {len(paper_responses)}í¸ ë°œê²¬")
        return paper_responses
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë…¼ë¬¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/download/pdf")
async def download_pdf(request: DownloadRequest, papers_data: List[PaperResponse]):
    """ë…¼ë¬¸ PDF ë‹¤ìš´ë¡œë“œ"""
    try:
        if request.paper_index >= len(papers_data):
            raise HTTPException(
                status_code=400, 
                detail=f"ì˜ëª»ëœ ë…¼ë¬¸ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤. 0-{len(papers_data)-1} ë²”ìœ„ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”."
            )
        
        selected_paper_data = papers_data[request.paper_index]
        
        # Paper ê°ì²´ ìƒì„± (ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ)
        paper = Paper(
            id=selected_paper_data.id,
            title=selected_paper_data.title,
            authors=selected_paper_data.authors,
            published_date=selected_paper_data.published_date,
            updated_date=selected_paper_data.updated_date,
            abstract=selected_paper_data.abstract,
            categories=selected_paper_data.categories,
            pdf_url=selected_paper_data.pdf_url,
            arxiv_url=selected_paper_data.arxiv_url,
            citation_count=selected_paper_data.citation_count,
            relevance_score=selected_paper_data.relevance_score
        )
        
        logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì‹œì‘: {paper.title}")
        
        # PDF ë‹¤ìš´ë¡œë“œ (ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©)
        filepath = scholar_agent.download_pdf(paper)
        
        if not filepath:
            raise HTTPException(status_code=500, detail="PDF ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
        
        return {
            "success": True,
            "message": "PDF ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "filepath": filepath,
            "filename": os.path.basename(filepath)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"PDF ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/multiagent/process", response_model=MultiAgentResponse)
async def process_with_multiagent(request: QuizRequest):
    """ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ PDF ì²˜ë¦¬ (ìš”ì•½/í€´ì¦ˆ/í•´ì„¤/TTS)"""
    try:
        if not os.path.exists(request.pdf_path):
            raise HTTPException(status_code=404, detail="ì§€ì •ëœ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"ë©€í‹°ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì‹œì‘: {request.pdf_path}")
        
        # ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì‹¤í–‰
        results = run_multi_agent(request.pdf_path)
        
        logger.info("ë©€í‹°ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        
        return MultiAgentResponse(
            success=True,
            message="ë©€í‹°ì—ì´ì „íŠ¸ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            summary=results.get("summary"),
            quiz=results.get("quiz"),
            explainer=results.get("explainer"),
            tts_file=f"industry_explainer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp3"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë©€í‹°ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë©€í‹°ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/workflow/start", response_model=WorkflowResponse)
async def start_workflow(request: WorkflowRequest, background_tasks: BackgroundTasks):
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œì‘ (ë…¼ë¬¸ ê²€ìƒ‰ â†’ PDF ë‹¤ìš´ë¡œë“œ â†’ ë©€í‹°ì—ì´ì „íŠ¸ ì²˜ë¦¬)"""
    try:
        workflow_id = f"workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì´ˆê¸°í™”
        workflow_status[workflow_id] = {
            "status": "started",
            "progress": 0,
            "current_step": "ë…¼ë¬¸ ê²€ìƒ‰",
            "message": "ì›Œí¬í”Œë¡œìš°ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "results": {}
        }
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        background_tasks.add_task(
            execute_workflow, 
            workflow_id, 
            request.domain, 
            request.additional_keywords,
            request.year_range,
            request.paper_index
        )
        
        return WorkflowResponse(
            success=True,
            message="ì›Œí¬í”Œë¡œìš°ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            workflow_id=workflow_id,
            papers=[],
            downloaded_pdf=None,
            quiz_generated=False,
            summary_generated=False,
            explainer_generated=False,
            podcast_created=False
        )
        
    except Exception as e:
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹œì‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì›Œí¬í”Œë¡œìš° ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

async def execute_workflow(workflow_id: str, domain: str, additional_keywords: Optional[str], 
                         year_range: int, paper_index: int):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©)"""
    try:
        # 1ë‹¨ê³„: ë…¼ë¬¸ ê²€ìƒ‰ (ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©)
        workflow_status[workflow_id].update({
            "status": "running",
            "progress": 20,
            "current_step": "ë…¼ë¬¸ ê²€ìƒ‰",
            "message": f"'{domain}' ë„ë©”ì¸ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        })
        
        papers = scholar_agent.fetch_papers(domain)
        
        if not papers:
            workflow_status[workflow_id].update({
                "status": "failed",
                "progress": 0,
                "current_step": "ë…¼ë¬¸ ê²€ìƒ‰",
                "message": "ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
            })
            return
        
        # 2ë‹¨ê³„: PDF ë‹¤ìš´ë¡œë“œ (ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©)
        workflow_status[workflow_id].update({
            "progress": 40,
            "current_step": "PDF ë‹¤ìš´ë¡œë“œ",
            "message": f"ë…¼ë¬¸ '{papers[paper_index].title}' PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        })
        
        filepath = scholar_agent.download_pdf(papers[paper_index])
        
        if not filepath:
            workflow_status[workflow_id].update({
                "status": "failed",
                "progress": 40,
                "current_step": "PDF ë‹¤ìš´ë¡œë“œ",
                "message": "PDF ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            })
            return
        
        # 3ë‹¨ê³„: ë©€í‹°ì—ì´ì „íŠ¸ ì²˜ë¦¬ (ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©)
        workflow_status[workflow_id].update({
            "progress": 60,
            "current_step": "ë©€í‹°ì—ì´ì „íŠ¸ ì²˜ë¦¬",
            "message": "PDFë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½/í€´ì¦ˆ/í•´ì„¤ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        })
        
        multiagent_results = run_multi_agent(filepath)
        
        # ì™„ë£Œ
        workflow_status[workflow_id].update({
            "status": "completed",
            "progress": 100,
            "current_step": "ì™„ë£Œ",
            "message": "ì „ì²´ ì›Œí¬í”Œë¡œìš°ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "results": {
                "papers_found": len(papers),
                "downloaded_pdf": filepath,
                "summary_generated": bool(multiagent_results.get("summary")),
                "quiz_generated": bool(multiagent_results.get("quiz")),
                "explainer_generated": bool(multiagent_results.get("explainer")),
                "podcast_created": bool(multiagent_results.get("explainer"))
            }
        })
        
        logger.info(f"ì›Œí¬í”Œë¡œìš° {workflow_id} ì™„ë£Œ")
        
    except Exception as e:
        workflow_status[workflow_id].update({
            "status": "failed",
            "current_step": "ì˜¤ë¥˜",
            "message": f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        })
        logger.error(f"ì›Œí¬í”Œë¡œìš° {workflow_id} ì‹¤í–‰ ì˜¤ë¥˜: {e}")

@app.get("/workflow/status/{workflow_id}", response_model=StatusResponse)
async def get_workflow_status(workflow_id: str):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ"""
    if workflow_id not in workflow_status:
        raise HTTPException(status_code=404, detail="ì›Œí¬í”Œë¡œìš°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    status_data = workflow_status[workflow_id]
    return StatusResponse(
        workflow_id=workflow_id,
        status=status_data["status"],
        progress=status_data["progress"],
        current_step=status_data["current_step"],
        message=status_data["message"],
        results=status_data.get("results")
    )

@app.get("/workflow/list")
async def list_workflows():
    """ì‹¤í–‰ ì¤‘ì¸ ì›Œí¬í”Œë¡œìš° ëª©ë¡"""
    return {
        "workflows": list(workflow_status.keys()),
        "count": len(workflow_status)
    }

@app.get("/papers/downloaded")
async def get_downloaded_papers():
    """ë‹¤ìš´ë¡œë“œëœ PDF íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
    try:
        if not downloaded_papers_dir.exists():
            return {"papers": [], "count": 0}
        
        pdf_files = list(downloaded_papers_dir.glob("*.pdf"))
        papers_info = []
        
        for pdf_file in pdf_files:
            stat = pdf_file.stat()
            papers_info.append({
                "filename": pdf_file.name,
                "filepath": str(pdf_file),
                "size": stat.st_size,
                "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
            })
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        papers_info.sort(key=lambda x: x["modified"], reverse=True)
        
        return {
            "papers": papers_info,
            "count": len(papers_info)
        }
        
    except Exception as e:
        logger.error(f"ë‹¤ìš´ë¡œë“œëœ ë…¼ë¬¸ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë…¼ë¬¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "features": "ë©€í‹°ì—ì´ì „íŠ¸ ê¸°ë°˜ ë…¼ë¬¸ ë¶„ì„ ì‹œìŠ¤í…œ"
    }

# ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.exception_handler(404)
async def not_found_handler(request, exc):
    return JSONResponse(
        status_code=404,
        content={"detail": "ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    )

@app.exception_handler(500)
async def internal_error_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={"detail": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}
    )

if __name__ == "__main__":
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "final:app",
        host="127.0.0.1",
        port=8000,
        reload=True,
        log_level="info"
    )
