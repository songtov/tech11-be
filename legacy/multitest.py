import os
import re
import tempfile
import requests
from datetime import datetime
from typing import List, Dict, Any, Optional, TypedDict

from dotenv import load_dotenv

load_dotenv()

# ========== LangChain / LangGraph ==========
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# LangGraph
from langgraph.graph import StateGraph, END

# TTS
from gtts import gTTS


# ==========================
# ìœ í‹¸: í…ìŠ¤íŠ¸ ì •ë¦¬(TTSìš©)
# ==========================
def clean_text(text: str) -> str:
    cleaned = re.sub(r"[#*>â€¢\-]+", " ", text)
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


# ==========================
# LLM / Embeddings íŒ©í† ë¦¬
# ==========================
def build_llm(use_mini: bool = True, temperature: float = 0.2):
    return AzureChatOpenAI(
        openai_api_version="2024-02-01",
        azure_deployment=(
            os.getenv("AOAI_DEPLOY_GPT4O_MINI")
            if use_mini
            else os.getenv("AOAI_DEPLOY_GPT4O")
        ),
        api_key=os.getenv("AOAI_API_KEY"),
        azure_endpoint=os.getenv("AOAI_ENDPOINT"),
        temperature=temperature,
    )


def build_embeddings():
    return AzureOpenAIEmbeddings(
        model=os.getenv("AOAI_DEPLOY_EMBED_3_LARGE"),
        openai_api_version="2024-02-01",
        api_key=os.getenv("AOAI_API_KEY"),
        azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    )


# ==========================
# PDF ë¡œë“œ & ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•
# ==========================
def load_pdf(path_or_url: str) -> List[Document]:
    if path_or_url.startswith("http://") or path_or_url.startswith("https://"):
        resp = requests.get(path_or_url, timeout=30)
        resp.raise_for_status()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(resp.content)
            tmp_path = tmp.name
        # íŒŒì¼ í•¸ë“¤ ë‹«íŒ ë’¤ ë¡œë”ê°€ ì—´ë„ë¡
        loader = PyMuPDFLoader(tmp_path)
        docs = loader.load()
        os.unlink(tmp_path)
    else:
        loader = PyMuPDFLoader(path_or_url)
        docs = loader.load()
    return docs


def build_vectorstore(
    docs: List[Document],
    embeddings,
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    splits = splitter.split_documents(docs)
    for d in splits:
        page = d.metadata.get("page", None)
        src = d.metadata.get("source", "")
        prefix = f"[source: {os.path.basename(src)} | page: {page}] "
        d.page_content = prefix + d.page_content
    vs = FAISS.from_documents(splits, embeddings)
    return vs


# ==========================
# ì²´ì¸: ìš”ì•½ / í€´ì¦ˆ / í•´ì„¤ / íŒì •
# ==========================
def make_summary_chain():
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë…¼ë¬¸ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë¬¸ì„œë¥¼ ì½ê³  ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ ê°„ê²°í•˜ê³  êµ¬ì¡°í™”ëœ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.

1) í•œ ì¤„ ìš”ì•½
2) ì—°êµ¬ ë°°ê²½ê³¼ ë¬¸ì œ ì •ì˜
3) í•µì‹¬ ê¸°ìˆ ê³¼ ë°©ë²•ë¡ 
4) ì£¼ìš” ê²°ê³¼ì™€ ì„±ëŠ¥
5) ê¸°ìˆ ì  ì‹œì‚¬ì ê³¼ í•œê³„
6) í•µì‹¬ í‚¤ì›Œë“œ

ë¬¸ì„œ ë‚´ìš©:
{document_content}
"""
    )
    return prompt | build_llm(use_mini=True) | StrOutputParser()


def make_quiz_chain():
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë…¼ë¬¸ ê¸°ë°˜ í€´ì¦ˆ ì œì‘ìì…ë‹ˆë‹¤. ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í•œêµ­ì–´ í€´ì¦ˆë¥¼ ì‘ì„±í•˜ì„¸ìš”.

- ì´ 5ë¬¸í•­: ê°ê´€ì‹/ì£¼ê´€ì‹/ì„œìˆ í˜• í˜¼í•©
- ê° ë¬¸í•­ë§ˆë‹¤ ì •ë‹µê³¼ í•´ì„¤ í¬í•¨
- ì €ì/ì—°ë„ ê°™ì€ ì§€ì—½ì  ì‚¬ì‹¤ì€ í”¼í•˜ê³ , í•µì‹¬ ê°œë… ì¤‘ì‹¬
- ë§ˆì§€ë§‰ì— 'ìƒê°í•´ë³¼ ì˜ê²¬ 3ê°€ì§€'ì™€ 'ì‹¤ë¬´ ì ìš© ë°©í–¥ 3ê°€ì§€' ì¶”ê°€

ë¬¸ì„œ ë‚´ìš©:
{document_content}
"""
    )
    return prompt | build_llm(use_mini=True) | StrOutputParser()


def make_explainer_chain():
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì „ë¬¸ í•´ì„¤ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ í•´ì„¤ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

êµ¬ì„±:
1) ë…¼ë¬¸ì˜ ìƒì„¸ ì„¤ëª…
2) ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ ì„¤ëª…
3) ì‚°ì—… í˜„ì¥ì—ì„œì˜ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ 2~3ê°€ì§€

ë¬¸ì„œ ë‚´ìš©:
{document_content}
"""
    )
    return prompt | build_llm(use_mini=False) | StrOutputParser()


def make_judge_chain():
    prompt = PromptTemplate.from_template(
        """ë‹¤ìŒ ìƒì„±ë¬¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.
ê¸°ì¤€: (A) ë¬¸ì„œ í•µì‹¬ ì£¼ì œë¥¼ ë¹ ì§ì—†ì´ ë‹¤ë£¨ì—ˆëŠ”ê°€, (B) ë…¼ë¦¬ì  ì¼ê´€ì„±, (C) ê³¼ë„í•œ í™˜ê°(ë¬¸ì„œì— ì—†ëŠ” ì£¼ì¥) ì—¬ë¶€.

ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥:
- YES: ì¶©ë¶„íˆ ì–‘í˜¸
- NO: ë¶ˆì¶©ë¶„ (ì¬ê²€ìƒ‰/ì¬ìƒì„± í•„ìš”)

í‰ê°€í•  í…ìŠ¤íŠ¸:
{generated}
"""
    )
    # íŒë‹¨ì€ ë³´ìˆ˜ì ìœ¼ë¡œ â†’ gpt-4oë¡œ íŒë‹¨
    return prompt | build_llm(use_mini=False, temperature=0.0) | StrOutputParser()


# ==========================
# ìƒíƒœ ì •ì˜ (LangGraph State)
# ==========================
class AgentState(TypedDict, total=False):
    # ì…ë ¥/ê³µìœ 
    vectorstore: Any
    query_summary: str
    query_quiz: str
    query_explainer: str
    k: int  # RAG ê²€ìƒ‰ ê°œìˆ˜

    # ì‚°ì¶œë¬¼
    summary: str
    quiz: str
    explainer: str

    # ë‚´ë¶€ ì‹ í˜¸
    judge_summary_ok: bool
    judge_quiz_ok: bool
    judge_explainer_ok: bool


# ==========================
# ê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ë“¤
# ==========================
def node_summarizer(state: AgentState) -> AgentState:
    vs = state["vectorstore"]
    k = state.get("k", 12)
    chunks = vs.similarity_search(
        state.get("query_summary", "summary overview of this document"), k=k
    )
    content = "\n\n".join([c.page_content for c in chunks])
    summary_chain = make_summary_chain()
    summary = summary_chain.invoke({"document_content": content})
    return {"summary": summary}


def node_quiz(state: AgentState) -> AgentState:
    vs = state["vectorstore"]
    k = state.get("k", 10)
    chunks = vs.similarity_search(
        state.get("query_quiz", "Generate exam questions based on this document"), k=k
    )
    content = "\n\n".join([c.page_content for c in chunks])
    quiz_chain = make_quiz_chain()
    quiz = quiz_chain.invoke({"document_content": content})
    return {"quiz": quiz}


def node_explainer(state: AgentState) -> AgentState:
    vs = state["vectorstore"]
    k = state.get("k", 15)
    chunks = vs.similarity_search(
        state.get("query_explainer", "detailed explanation with industry applications"),
        k=k,
    )
    content = "\n\n".join([c.page_content for c in chunks])
    explainer_chain = make_explainer_chain()
    explainer = explainer_chain.invoke({"document_content": content})
    return {"explainer": explainer}


def node_judge_summary(state: AgentState) -> AgentState:
    judge = make_judge_chain()
    verdict = judge.invoke({"generated": state.get("summary", "")}).strip().upper()
    return {"judge_summary_ok": verdict.startswith("YES")}


def node_judge_quiz(state: AgentState) -> AgentState:
    judge = make_judge_chain()
    verdict = judge.invoke({"generated": state.get("quiz", "")}).strip().upper()
    return {"judge_quiz_ok": verdict.startswith("YES")}


def node_judge_explainer(state: AgentState) -> AgentState:
    judge = make_judge_chain()
    verdict = judge.invoke({"generated": state.get("explainer", "")}).strip().upper()
    return {"judge_explainer_ok": verdict.startswith("YES")}


def cond_on_summary(state: AgentState) -> str:
    """
    ì¡°ê±´ë¶€ ì—£ì§€ ë¼ìš°íŒ…:
    - ìš”ì•½ ë¯¸í¡ â†’ kë¥¼ ì¦ê°€ì‹œí‚¤ê³  summarizer ì¬ì‹œë„
    - ì¶©ë¶„ â†’ quizë¡œ ì§„í–‰
    """
    if state.get("judge_summary_ok", True):
        return "ok"
    # ë¯¸í¡í•˜ë©´ k + 4 (ìƒí•œì„  40)
    new_k = min(40, state.get("k", 12) + 4)
    state["k"] = new_k
    return "retry"


def cond_on_quiz(state: AgentState) -> str:
    if state.get("judge_quiz_ok", True):
        return "ok"
    new_k = min(40, state.get("k", 12) + 4)
    state["k"] = new_k
    return "retry"


def cond_on_explainer(state: AgentState) -> str:
    if state.get("judge_explainer_ok", True):
        return "ok"
    new_k = min(40, state.get("k", 12) + 4)
    state["k"] = new_k
    return "retry"


def node_tts(state: AgentState) -> AgentState:
    script = state.get("explainer", "")
    if not script:
        return {}
    script_clean = clean_text(script)
    tts = gTTS(text=script_clean, lang="ko")
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_mp3 = f"industry_explainer_{ts}.mp3"
    tts.save(out_mp3)
    print(f"ğŸ§ TTS ì €ì¥ ì™„ë£Œ: {out_mp3}")
    return {}


# ==========================
# ì›Œí¬í”Œë¡œìš° êµ¬ì„± (LangGraph)
# ==========================
def build_workflow():
    graph = StateGraph(AgentState)

    # ë…¸ë“œ ë“±ë¡
    graph.add_node("summarizer", node_summarizer)
    graph.add_node("judge_summary", node_judge_summary)
    graph.add_node("quiz", node_quiz)
    graph.add_node("judge_quiz", node_judge_quiz)
    graph.add_node("explainer", node_explainer)
    graph.add_node("judge_explainer", node_judge_explainer)
    graph.add_node("tts", node_tts)

    # ì§„ì…ì 
    graph.set_entry_point("summarizer")

    # summarizer â†’ judge â†’ (retry: summarizer, ok: quiz)
    graph.add_edge("summarizer", "judge_summary")
    graph.add_conditional_edges(
        "judge_summary",
        cond_on_summary,
        {
            "retry": "summarizer",
            "ok": "quiz",
        },
    )

    # quiz â†’ judge â†’ (retry: quiz, ok: explainer)
    graph.add_edge("quiz", "judge_quiz")
    graph.add_conditional_edges(
        "judge_quiz",
        cond_on_quiz,
        {
            "retry": "quiz",
            "ok": "explainer",
        },
    )

    # explainer â†’ judge â†’ (retry: explainer, ok: tts)
    graph.add_edge("explainer", "judge_explainer")
    graph.add_conditional_edges(
        "judge_explainer",
        cond_on_explainer,
        {
            "retry": "explainer",
            "ok": "tts",
        },
    )

    # ë§ˆì§€ë§‰ tts í›„ ì¢…ë£Œ
    graph.add_edge("tts", END)

    return graph.compile()


# ==========================
# ì‹¤í–‰ í•¨ìˆ˜
# ==========================
def run_multi_agent(pdf_path_or_url: str):
    print(f"\nğŸ¯ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path_or_url}")
    docs = load_pdf(pdf_path_or_url)
    print(f"âœ… PDF ë¡œë“œ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ")

    embeddings = build_embeddings()
    vs = build_vectorstore(docs, embeddings)
    print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ")

    workflow = build_workflow()
    init_state: AgentState = {
        "vectorstore": vs,
        "k": 12,
        "query_summary": "summary overview of this document",
        "query_quiz": "Generate exam questions based on this document",
        "query_explainer": "detailed explanation with industry applications",
    }

    # ì‹¤í–‰
    final_state = workflow.invoke(init_state)

    # ì‚°ì¶œë¬¼ ì €ì¥
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    if final_state.get("summary"):
        with open(f"summary_{ts}.txt", "w", encoding="utf-8") as f:
            f.write(final_state["summary"])
        print(f"ğŸ“ ìš”ì•½ ì €ì¥: summary_{ts}.txt")

    if final_state.get("quiz"):
        with open(f"quiz_{ts}.txt", "w", encoding="utf-8") as f:
            f.write(final_state["quiz"])
        print(f"ğŸ“ í€´ì¦ˆ ì €ì¥: quiz_{ts}.txt")

    if final_state.get("explainer"):
        with open(f"explainer_{ts}.txt", "w", encoding="utf-8") as f:
            f.write(final_state["explainer"])
        print(f"ğŸ“ í•´ì„¤ ì €ì¥: explainer_{ts}.txt")

    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    return final_state


# ==========================
# main
# ==========================
if __name__ == "__main__":
    # Windows ê²½ë¡œ ì£¼ì˜: ë°±ìŠ¬ë˜ì‹œëŠ” r"..." í˜¹ì€ ìŠ¬ë˜ì‹œ ì‚¬ìš©
    PDF_INPUT = r"C:/Users/Administrator/Desktop/ì—°ìŠµ/2505.18397v3.pdf"
    run_multi_agent(PDF_INPUT)
